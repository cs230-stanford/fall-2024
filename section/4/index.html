<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, minimum-scale=1.0">
<title>Section 4 (Week 4)</title>

	<meta name="description" content="Xavier Initialization and Regularization">


<link rel="icon" href="/favicon.ico" type="image/x-icon">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,400i,500,500i,700,700i|Noto+Sans:400,400i,700,700i|Source+Code+Pro&amp;subset=latin-ext">
<link rel="stylesheet" href="/doks-theme/assets/css/style.css">
<script type="text/javascript" async
  src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script>
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-124237072-3', 'auto');
	ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
	</head>
	<body class="blue" data-spy="scroll" data-target=".js-scrollspy">
		
	<div class="offcanvas visible-xs">
		<ul class="offcanvas__nav">
			
				<li><a href="/syllabus">Syllabus</a></li>
			
				<li><a href="/officehours">Office Hours</a></li>
			
				<li><a href="/project">Project</a></li>
			
				<li><a href="/section">Section</a></li>
			
				<li><a href="/lecture">Lecture</a></li>
			
				<li><a href="/blog">Blog</a></li>
			
				<li><a href="/faq">FAQ</a></li>
			
		</ul><!-- /.offcanvas__nav -->
	</div><!-- /.offcanvas -->



	<header class="site-header">
		<div class="container">
			<div class="row">
				<div class="col-xs-12">
					
						<a href="/" class="site-header__logo">
							<img src="/doks-theme/assets/images/layout/logo.png">
							CS230
						</a>
					
					
						<ul class="site-header__nav hidden-xs">
							
								<li><a href="/syllabus">Syllabus</a></li>
							
								<li><a href="/officehours">Office Hours</a></li>
							
								<li><a href="/project">Project</a></li>
							
								<li><a href="/section">Section</a></li>
							
								<li><a href="/lecture">Lecture</a></li>
							
								<li><a href="/blog">Blog</a></li>
							
								<li><a href="/faq">FAQ</a></li>
							
						</ul><!-- /.site-header__nav -->
						<button class="offcanvas-toggle visible-xs">
							<span></span>
							<span></span>
							<span></span>
						</button><!-- /.offcanvas-toggle -->
					
				</div><!-- /.col -->
			</div><!-- /.row -->
		</div><!-- /.container -->
	</header><!-- /.site-header -->


		<div class="hero-subheader">
			<div class="container">
				<div class="row">
					<div class="col-md-7">
						<div class="align-container" data-mh>
							<div class="align-inner">
								
									<h1 class="hero-subheader__title">Section 4 (Week 4)</h1>
								
								
									<p class="hero-subheader__desc">Xavier Initialization and Regularization</p>
								
								
								
							</div><!-- /.align-inner -->
						</div><!-- /.align-container -->
					</div><!-- /.col -->
						<div class="col-md-4 col-md-offset-1 hidden-xs hidden-sm">
							<div class="align-container" data-mh>
								<div class="align-inner">
									<div class="hero-subheader__author">
										<h4>Instructors</h4>
                                                                               
                                                                               
                                                                                    <div class="instructor">
                                        <a target="_blank" href="">
                    <img class="headshot" src="/doks-theme/assets/images/headshot/kian.jpg" style="text-align:center;">
                <div style="text-align:center;">Kian Katanforoosh<br>Instructor</div>
	</a>
        </div>
    

                                                                               
                                                                                    <div class="instructor">
                                        <a target="_blank" href="">
                    <img class="headshot" src="/doks-theme/assets/images/headshot/andrew.jpg" style="text-align:center;">
                <div style="text-align:center;">Andrew Ng<br>Instructor</div>
	</a>
        </div>
    

                                                                               

                                                                               

                                                                               

										<h4>Time and Location</h4>
                                                                                <p>In person lectures are on Tuesdays 11:30am-1:20pm. <br> Hewlett Teaching Center 200</p>
									</div><!-- /.hero-subheader__author -->
								</div><!-- /.align-inner -->
							</div><!-- /.align-container -->
						</div><!-- /.col -->
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.hero-subheader -->
		<div class="section">
			<div class="container">
				<div class="row">
					
					<div class="col-md-7">
						<div class="content">
							<p>Training a deep neural network involves finding the right values for the weights: we would like to end with a model with good <strong>generalization error</strong>. Our model should perform well on the full, real-world data distribution, and should neither underfit nor overfit.</p>

<p>In this section, we’ll analyze two methods, initialization and regularization, and show how they help us train models more effectively.</p>

<h1 id="xavier-initialization">Xavier Initialization</h1>

<p>Last week, we discussed backpropagation and gradient descent for deep learning models. All deep learning optimization methods involve an initialization of the weight parameters.</p>

<p>Let’s explore the <strong><a href="https://www.deeplearning.ai/ai-notes/initialization/index.html" target="_blank">first visualization in this article</a></strong> to gain some intuition on the effect of different initializations.</p>

<p><strong>What makes a good or bad initialization? How can different magnitudes of initializations lead to exploding and vanishing gradients?</strong></p>

<p><strong>If we initialize weights to all zeros or the same value, what problem arises?</strong></p>

<table class="image">
<caption align="bottom"><small>Visualizing the effects of different initializations. (<a href="https://www.deeplearning.ai/ai-notes/initialization/index.html">source</a>)</small></caption> 
<tr><td><img src="/doks-theme/assets/images/section/4/viz.png" style="width: 100%" /></td></tr>
</table>

<p>The goal of Xavier Initialization is to initialize the weights such that the variance of the activations are the same across every layer. This constant variance helps prevent the gradient from exploding or vanishing.</p>

<p>To help derive our initialization values, we will make the following <strong>simplifying assumptions</strong>:</p>

<ul>
  <li>Weights and inputs are centered at zero</li>
  <li>Weights and inputs are independent and identically distributed</li>
  <li>Biases are initialized as zeros</li>
  <li>We use the tanh() activation function, which is approximately linear with small inputs: \(Var(a^{[l]}) \approx Var(z^{[l]})\)</li>
</ul>

<p><strong>Let’s derive Xavier Initialization now, step by step.</strong></p>

<p>Our full derivation gives us the following initialization rule, which we apply to all weights:</p>

\[W^{[l]}_{i,j} = \mathcal{N}\left(0,\frac{1}{n^{[l-1]}}\right)\]

<table class="image">

<tr><td><img src="/doks-theme/assets/images/section/5/proof.png" style="width: 100%" /></td></tr>
</table>

<p>Xavier initialization is designed to work well with tanh or sigmoid activation functions. For ReLU activations, look into He initialization, which follows a very similar derivation.</p>

<h1 id="l1-and-l2-regularization">L1 and L2 Regularization</h1>

<p>We know that \(L_1\) regularization encourages sparse weights (many zero values), and that \(L_2\) regularization encourages small weight values, but <strong>why does this happen?</strong></p>

<p>Let’s consider some cost function \(J(w_1,\dots,w_l)\), a function of weight matrices \(w_1,\dots,w_l\). Let’s define the following two regularized cost functions:</p>

\[\begin{align} 
J_{L_1}(w_1,\dots,w_l) &amp;= J(w_1,\dots,w_l) + \lambda\sum_{i=1}^l|w_i|\\
J_{L_2}(w_1,\dots,w_l) &amp;= J(w_1,\dots,w_l) + \lambda\sum_{i=1}^l||w_i||^2
\end{align}\]

<p><strong>Let’s derive the gradient updates for these cost functions.</strong></p>

<p>The update for \(w_i\) when using \(J_{L_1}\) is:</p>

\[w_i^{k+1} = w_i^{k} - \underbrace{\alpha\lambda sign(w_i)}_{L_1 \text{ penalty}} - \alpha\frac{\partial J}{\partial w_i}\]

<p>The update for \(w_i\) when using \(J_{L_2}\) is:</p>

\[w_i^{k+1} = w_i^{k} - \underbrace{2\alpha\lambda w_i}_{L_2 \text{ penalty}}- \alpha\frac{\partial J}{\partial w_i}\]

<p><strong>What do you notice that is different between these two update rules, and how does it affect optimization? What effect does the hyperparameter \(\lambda\) have?</strong></p>

<table class="image">
<caption align="bottom"><small>A histogram of weight values for an unregularized (red) and L1 regularized (blue left) and L2 regularized (blue right) network. (<a href="https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html">source</a>)</small></caption> 
<tr><td><img src="/doks-theme/assets/images/section/6/histogram.jpeg" style="width: 100%" /></td></tr>
</table>

<p>The different effects of \(L_1\) and \(L_2\) regularization on the optimal parameters are an artifact of the different ways in which they change the original loss landscape. In the case of two parameters (\(w_1\) and \(w_2\)), we can <a href="https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html" target="_blank"><strong>visualize this</strong></a>.</p>

<table class="image">
<caption align="bottom"><small>The landscape of a two parameter loss function with L1 regularization (left) and L2 regularization (right). (<a href="https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html">source</a>)</small></caption> 
<tr><td><img src="/doks-theme/assets/images/section/6/loss.jpeg" style="width: 100%" /></td></tr>
</table>

<p>For a deeper dive into regularization, take a look at this longer <a href="https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html" target="_blank">blog post</a>, as well as Chapter 3 of <a href="https://web.stanford.edu/~hastie/ElemStatLearn/" target="_blank">The Elements of Statistical Learning</a>.</p>


						</div><!-- /.content -->
					</div><!-- /.col -->
					<div class="col-md-4 col-md-offset-1">
						<div class="sections-list-wrapper">
							<div class="sections-list js-sections js-affix js-scrollspy hidden-xs hidden-sm"></div><!-- /.sections-list -->
						</div>
					</div><!-- /.col -->
					
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.section -->
		
		<div class="js-footer-area">
			
			
	<footer class="site-footer">
		<div class="container">
			<div class="row">
				<div class="col-sm-6">
					
					
						<!-- <hr> -->
						<p class="site-footer__copyright">Copyright &copy; 2024. - Stanford University <br>All rights reserved.</p>
					
				</div><!-- /.col -->
				
			</div><!-- /.row -->
		</div><!-- /.container -->
	</footer><!-- /.site-footer -->


<script src="/doks-theme/assets/js/vendor/jquery.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/affix.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/scrollspy.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/matchHeight.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/scripts.min.js"></script>

		</div><!-- /.js-footer-area -->
	</body>
</html>
